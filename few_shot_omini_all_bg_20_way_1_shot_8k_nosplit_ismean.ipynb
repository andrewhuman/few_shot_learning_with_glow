{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glow import thops\n",
    "from glow import modules\n",
    "from glow import models\n",
    "from glow.config import JsonConfig\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative\n",
    "path_total = '/home/hyshuai/workspace/dataset/images_background'\n",
    "\n",
    "dataset_negative = []\n",
    "for path_p in os.listdir(path_total):\n",
    "        path_p_join = os.path.join(path_total,path_p)\n",
    "        for path_c in os.listdir(path_p_join):\n",
    "            path_real = os.path.join(path_p_join,path_c)\n",
    "            class_name = path_real\n",
    "            # image_paths = get_image_paths(path_real)\n",
    "            images = get_images_frompath(path_real)\n",
    "            print(class_name,\" : \",len(images))\n",
    "            dataset_negative.append(ImageObject(class_name,images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# positive\n",
    "\n",
    "path_positive_total = '/home/hyshuai/workspace/dataset/images_evaluation'\n",
    "class_name_list = []\n",
    "for path_p in os.listdir(path_positive_total):\n",
    "    path_p_join = os.path.join(path_positive_total,path_p)\n",
    "    for path_c in os.listdir(path_p_join):\n",
    "        path_real = os.path.join(path_p_join,path_c)\n",
    "        class_name = path_real\n",
    "        class_name_list.append(str(class_name))\n",
    "\n",
    "# class_name = np.array(class_name)\n",
    "# print(class_name_list)\n",
    "np.random.seed(66) # 切换类试下\n",
    "shufindex = np.random.permutation(len(class_name_list))\n",
    "# print(shufindex)\n",
    "class_positive = np.array(class_name_list)[shufindex][:20] #取5个类做测试 5-way\n",
    "print(class_positive)\n",
    "\n",
    "dataset_train_positive =[]\n",
    "dataset_test_positive = []\n",
    "for class_path in class_positive :\n",
    "    images = get_images_frompath_random(class_path) # 固定随机抽样是否有问题\n",
    "    print(class_path, \" : \", len(images))\n",
    "    dataset_train_positive.append(ImageObject(class_path, images[:1])) # 1-shot\n",
    "    dataset_test_positive.append(ImageObject(class_path, images[1:]))\n",
    "\n",
    "\n",
    "\n",
    "print(len(dataset_train_positive[2].imgs))\n",
    "print(len(dataset_test_positive[2].imgs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label:\n",
    "\n",
    "class_total_num = len(dataset_negative) + len(dataset_train_positive)\n",
    "print(\"class_total = \",class_total_num)\n",
    "np.random.seed(66) \n",
    "shufindex = np.random.permutation(class_total_num)\n",
    "for index, value in enumerate(dataset_negative):\n",
    "    value.label = shufindex[index]\n",
    "    print(value.name,  value.label)\n",
    "\n",
    "for index, value in enumerate(dataset_train_positive):\n",
    "    value.label = shufindex[index+len(dataset_negative)]\n",
    "    dataset_test_positive[index].label = shufindex[index+len(dataset_negative)]\n",
    "    print(value.name, value.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_class_index = [122,51,631,828,20]\n",
    "test_class_index = [data.label for data in dataset_train_positive]\n",
    "print(test_class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_test_positive[0].name)\n",
    "# np.random.shuffle(dataset_test_positive)\n",
    "# print(dataset_test_positive[0].name)\n",
    "# imgs_show(np.squeeze( dataset_test_positive[0].imgs),1,5) \n",
    "# imgs_show(np.squeeze(dataset_negative[1].imgs),4,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_negative[0].imgs[2].shape)\n",
    "# # print(im[15])\n",
    "# # print(im.shape)\n",
    "# print(dataset_negative[0].imgs[0].shape)\n",
    "x = [data.imgs for data in dataset_train_positive] \n",
    "print(x[0][0].shape)\n",
    "img_show(np.squeeze(x[0]))\n",
    "img_show(np.squeeze(x[1]))\n",
    "img_show(np.squeeze(x[2]))\n",
    "img_show(np.squeeze(x[3]))\n",
    "img_show(np.squeeze(x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_perfect(dataset_train_positive,dataset_negative,p_size,n_size):\n",
    "    \"\"\"\n",
    "    final batch_size = p_size + n_size * 2\n",
    "    every dataset_negative should have same imgs leng\n",
    "    \"\"\"\n",
    "    #shuffle\n",
    "    np.random.shuffle(dataset_train_positive)\n",
    "    np.random.shuffle(dataset_negative)\n",
    "    for data in dataset_negative:\n",
    "        np.random.shuffle(data.imgs)\n",
    "    for data in dataset_train_positive:\n",
    "        np.random.shuffle(data.imgs)\n",
    "    \n",
    "    \n",
    "    batch_negative = n_size * 2\n",
    "#     print('batch_negative ',batch_negative)\n",
    "    len_n_class = len(dataset_negative)\n",
    "#     print('len_n_class ',len_n_class)\n",
    "    len_p_class = len(dataset_train_positive)\n",
    "#     print('len_p_class ',len_p_class)\n",
    "    len_n_imgs = len(dataset_negative[0].imgs)\n",
    "#     print('len_n_imgs ',len_n_imgs)\n",
    "    len_p_imgs = len(dataset_train_positive[0].imgs)\n",
    "    \n",
    "    negative_number = 0\n",
    "    \n",
    "    \n",
    "#     shuffindex = np.random.permutation(len_n)\n",
    "# #     print(shuffindex)\n",
    "#     np.random.shuffle(dataset_train_positive)\n",
    "#     np.random.shuffle(dataset_negative)\n",
    "    \n",
    "    for data in dataset_negative:\n",
    "        negative_number += len(data.imgs)\n",
    "#         np.random.shuffle(data.imgs)\n",
    "#     print('negative_number ',negative_number)\n",
    "    batch_total = negative_number  // batch_negative\n",
    "#     print('batch_total ',batch_total)\n",
    "    i_n_class = 0\n",
    "    i_n_img = 0\n",
    "    i_p_class = 0\n",
    "    i_p_img = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    for b in range(batch_total):\n",
    "        train_x =[]\n",
    "        train_y = []\n",
    "        create_negative_imgs = 0\n",
    "        while create_negative_imgs <= (batch_total * batch_negative) and create_negative_imgs < batch_negative:\n",
    "            \n",
    "            # while #如果一次循环没够，需要再次循环\n",
    "            if i_n_class >= len_n_class:\n",
    "                    i_n_class = 0\n",
    "                    i_n_img +=1\n",
    "                    if (i_n_img+1) *2 >len_n_imgs:\n",
    "                        i_n_img = 0\n",
    "#                     print('if i_n_class >= len_n_class i_n_class ',i_n_class)\n",
    "#                     print('if i_n_class >= len_n_class i_n_img ',i_n_img)\n",
    "            \n",
    "            for i in range(i_n_class,len_n_class):\n",
    "                    \n",
    "#                 print('for i in range(i_n_class i_n_class ',i_n_class)\n",
    "#                 print('for i in range(i_n_class i_n_img ',i_n_img)\n",
    "                train_x += dataset_negative[i_n_class].imgs[i_n_img * 2 : (i_n_img+1) *2]\n",
    "                train_y += [dataset_negative[i_n_class].label] * 2\n",
    "                \n",
    "                i_n_class+=1\n",
    "                if i_n_class >= len_n_class:\n",
    "                    i_n_class = 0\n",
    "                    i_n_img +=1\n",
    "                    if (i_n_img+1) *2 > len_n_imgs:\n",
    "                        i_n_img = 0\n",
    "#                     print('if i_n_class >= len_n_class i_n_class ',i_n_class)\n",
    "#                     print('if i_n_class >= len_n_class i_n_img ',i_n_img)\n",
    "            \n",
    "                create_negative_imgs +=2\n",
    "                if create_negative_imgs >= batch_negative:\n",
    "#                     print(\"create_negative_imgs ok \",create_negative_imgs)\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "        create_positive_imgs = 0    \n",
    "        while create_positive_imgs < p_size:\n",
    "            for i in range(i_p_class,len_p_class):\n",
    "#                 print('for i in range(i_p_class     ',i_p_class)\n",
    "#                 print('for i in range(i_p_img  ',i_p_img)\n",
    "                \n",
    "                train_x += dataset_train_positive[i_p_class].imgs[i_p_img:(i_p_img +1)]\n",
    "                train_y += [dataset_train_positive[i_p_class].label]\n",
    "                \n",
    "                i_p_class+=1\n",
    "                if i_p_class >= len_p_class:\n",
    "                    i_p_class = 0\n",
    "                    i_p_img +=1\n",
    "                    if (i_p_img +1) > len_p_imgs:\n",
    "                        i_p_img = 0\n",
    "#                     print('if i_p_class >= len_p_class i_p_class ',i_p_class)\n",
    "#                     print('if i_p_class >= len_p_class i_p_img ',i_p_img)\n",
    "                \n",
    "                create_positive_imgs+=1\n",
    "                if create_positive_imgs >= p_size:\n",
    "#                     print(\"create_positive_imgs ok \",create_positive_imgs)\n",
    "                    break\n",
    "        \n",
    "        shuffindex_out = np.random.permutation(len(train_y))\n",
    "#         print(train_x[0].shape)\n",
    "        train_x = np.stack(train_x,axis = 0)\n",
    "#         print('after concat : ',train_x[0].shape)\n",
    "        train_x = train_x[shuffindex_out]\n",
    "        train_y = np.array(train_y)[shuffindex_out]\n",
    "        \n",
    "        yield train_x,train_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataaaaaaa = get_next_perfect(dataset_train_positive,dataset_negative,8,126)\n",
    "# for x ,y in dataaaaaaa:\n",
    "#     print(y)\n",
    "#     print('x o shape ' ,x.shape,y.shape)\n",
    "#     print(\"one batch is over\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnext_test(dataset_test_positive,test_class_index):\n",
    "    \n",
    "    for data in dataset_test_positive:\n",
    "        x = np.stack(data.imgs,axis =0)\n",
    "        y = np.array([test_class_index.index(data.label)] * len(data.imgs))\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataaaaaaa = getnext_test(dataset_test_positive,test_class_index)\n",
    "# for x ,y in dataaaaaaa:\n",
    "#     print(y)\n",
    "#     print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_show_norm(imgs,row,col):\n",
    "    fig,ax = plt.subplots(nrows=row,ncols=col,sharex=True,sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    for i in range(row*col):\n",
    "        img = imgs[i]\n",
    "        min = np.min(img)\n",
    "#         print('min = ',min)\n",
    "        img = np.subtract(img,min) # 0->\n",
    "        max = np.max(img)\n",
    "#         print('max =',max)\n",
    "        img = np.divide(img,max)\n",
    "        \n",
    "        ax[i].imshow(img,cmap='Greys', interpolation='nearest')\n",
    "    \n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_norm(img):\n",
    "    min_ = np.min(img)\n",
    "#     print('min = ',min_)\n",
    "    img = np.subtract(img,min_) # 0->\n",
    "    max_ = np.amax(a = img,keepdims=False)\n",
    "#     print('max =',max_)\n",
    "    img = np.divide(img,max_)  # 0->1\n",
    "#     print(img)\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img,cmap='Greys',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(img):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img,cmap='Greys',interpolation='nearest')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_show(imgs,row,col):\n",
    "    fig,ax = plt.subplots(nrows=row,ncols=col,sharex=True,sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    for i in range(row*col):\n",
    "        img = imgs[i]\n",
    "        \n",
    "        ax[i].imshow(img,cmap='Greys', interpolation='nearest')\n",
    "    \n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_index_logical_equal(inputs,indexs):\n",
    "    logic_result = (inputs==indexs[0])\n",
    "    for value in indexs:\n",
    "        logic_result = np.logical_or(inputs == value,logic_result)\n",
    "    return logic_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_digits = class_total_num\n",
    "net_auto = models.Glow(JsonConfig(\"hparams/omni_all_bg.json\"),test_class_index,is_mean=True,K=8,y_classes=class_total_num,is_split_2d=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in net_auto.parameters():\n",
    "#     print(param.data.size())\n",
    "net_auto.load_state_dict(torch.load('model_glow_ominst_all_bg_8k_20_way_778.pt'),strict=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# net_auto = torch.load('model_autoEncode5.pkl')\n",
    "# net_auto.load_state_dict(torch.load('model_glow_ominst19000.pt'))\n",
    "# net_auto.load_state_dict(torch.load('model_glow_ominst_no_split2d_4k_4316.pt'))\n",
    "# net_auto.load_state_dict(torch.load('model_glow_ominst_no_split2d_8k_9000.pt'))\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(666)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "net_auto.to(device)\n",
    "net_auto.float()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net_auto.parameters(),lr=0.0001,weight_decay=1e-5) # decoder don't weight_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train_nor\n",
    "batch_size = 512\n",
    "net_auto.train()\n",
    "for epoch in range(30001):\n",
    "    #train:\n",
    "    \n",
    "    datas =  get_next_perfect(dataset_train_positive,dataset_negative,12,250)\n",
    "#     datas = getnext(dataset_train_positive,dataset_negative)\n",
    "    \n",
    "    trainloss_g = 0\n",
    "    trainloss_c = 0\n",
    "    \n",
    "    for x,y in datas:\n",
    "        inputs ,lables = torch.from_numpy(x).float().to(device),\\\n",
    "            torch.from_numpy(y).long().to(device)\n",
    "#         print(torch.sum(lables)) # = 1167\n",
    "        \n",
    "        y_onehot = torch.FloatTensor(batch_size, class_total_num).to(device)\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, lables.view(-1,1), 1)\n",
    "#         print('y_onehot:', y_onehot)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         z, det, y_logits = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "        \n",
    "        z, det = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "        classify = net_auto.class_flow(z)\n",
    "        \n",
    "#         print(z.size())\n",
    "#         print(det)\n",
    "        loss_g = models.Glow.loss_generative(det)\n",
    "        loss_c = models.Glow.loss_class(y_logits,lables)\n",
    "        print('loss_g = ',loss_g.item())\n",
    "        print('loss_c =  ',loss_c.item())\n",
    "        loss = loss_g + loss_c * 0.2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(net_auto.parameters(), 5)\n",
    "        torch.nn.utils.clip_grad_norm_(net_auto.parameters(), 100)\n",
    "        \n",
    "        \n",
    "        trainloss_g += loss_g.item()\n",
    "        trainloss_c += loss_c.item()\n",
    "#         print(\"net_auto.conv_d1 grad:\",net_auto.conv_d1.weight.grad)\n",
    "        \n",
    "#         print(\"after net_auto.conv_1 grad:\",net_auto.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('%d  gloss: %.3f, class loss:%.3f' % (epoch ,  trainloss_g,trainloss_c))\n",
    "    \n",
    "    if epoch % 200 == 0 and epoch >0:\n",
    "        torch.save(net_auto.state_dict(),'model_glow_ominst_all_bg_8k_20_way_'+str(epoch)+'.pt')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(666)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "net_auto.to(device)\n",
    "net_auto.float()\n",
    "net_auto.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "test_datas = getnext_test(dataset_test_positive,test_class_index)\n",
    "for x ,y in test_datas:\n",
    "        inputs, labels = Variable(torch.from_numpy(x).float()).cuda(), Variable(torch.from_numpy(y).long()).cuda()\n",
    "        print(labels)\n",
    "        \n",
    "        z, det = net_auto(x=inputs, y_onehot=None)\n",
    "        classify = net_auto.class_flow(z)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         _,classify = net_auto(inputs)\n",
    "        test_loss += F.nll_loss(classify, labels, size_average=False).item() # sum up batch loss\n",
    "#         print(classify)\n",
    "        pred = classify.max(1,keepdim=True)[1]\n",
    "#         print(pred)\n",
    "        \n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "#         print(correct)\n",
    "print(correct)\n",
    "print(correct * 1.0 / (19 * 20))\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_backgroud = np.array([data.imgs[0] for data in dataset_negative])\n",
    "label_backgroud = np.array([data.label for data in dataset_negative])\n",
    "x_positive = np.array([data.imgs[0] for data in dataset_train_positive])\n",
    "label_positive = np.array([data.label for data in dataset_train_positive])\n",
    "\n",
    "images_train_total = np.concatenate((x_backgroud ,x_positive)) \n",
    "label_total = np.concatenate((label_backgroud , label_positive))\n",
    "print(images_train_total.shape)\n",
    "print(label_total)\n",
    "print(shufindex)\n",
    "imgs_show(np.squeeze(images_train_total[0:20]),4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_positive)\n",
    "imgs_show(np.squeeze(x_positive),4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(images_train_total)\n",
    "# lables = np.array([index for index in range(class_total_num)])\n",
    "lables = torch.from_numpy(label_total).long().to(device)\n",
    "print(\"get_log_mean_var ,labels = \", lables)\n",
    "y_onehot = torch.FloatTensor(batch_size, class_total_num).cuda()\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, lables.view(-1, 1), 1)\n",
    "\n",
    "mean,var = net_auto.get_log_mean_var(batch_size,y_onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = torch.exp(var.cpu()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(std[-70:-20,2,0,0])\n",
    "print(std[-20:,2,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = mean.cpu().detach().numpy()\n",
    "# mean_y = np.ones_like(mean_x)\n",
    "color = (mean_x + 1 ) * 150\n",
    "print(mean_x[-200:-20,0,0,0])\n",
    "print(mean_x[-20:,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_total = np.mean(mean_x,axis=0)\n",
    "print('u_total =  ',u_total[:64,0,0])\n",
    "std_mean = np.mean(np.square(std),axis=0)\n",
    "print('std_mean =  ',std_mean[:10,0,0])\n",
    "ui_squ_mean = np.mean(np.square(mean_x),axis=0)\n",
    "print('ui_squ_mean =  ',ui_squ_mean[:10,0,0])\n",
    "u_total_sque = np.square(u_total)\n",
    "print('u_total_sque =  ',u_total_sque[:10,0,0])\n",
    "std_squ_total = std_mean + ui_squ_mean - u_total_sque\n",
    "\n",
    "print('std_squ_total =  ',std_squ_total[:10,0,0])\n",
    "print('std__total = ',np.sqrt(std_squ_total)[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(200,100))\n",
    "plt.scatter(x = mean_x[:,0,0,0],y = np.zeros_like(mean_x[:,0,0,0]),s=75,c =color[:,0,0,0],  alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "ones_tensor = torch.ones((1, 32, 32)).float().cuda()\n",
    "\n",
    "from scipy import misc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_auto.load_state_dict(torch.load('model_glow_ominst2000.pt'))\n",
    "\n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# torch.manual_seed(666)\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# net_auto.to(device)\n",
    "# net_auto.float()\n",
    "# net_auto.load_state_dict(torch.load('model_glow_ominst_no_split2d_no_z_mean_no_add_normal1223.pt'))\n",
    "\n",
    "# net_auto.load_state_dict(torch.load('model_glow_ominst_no_split2d_8k_9000.pt'))\n",
    "# net_auto.train()\n",
    "# torch.save(net_auto.state_dict(),'model_glow_ominst_all_bg_8k_20_way_778.pt')\n",
    "\n",
    "def sample_and_save(sample_p,p,sampe_index_num):\n",
    "    \n",
    "    sample_n = np.random.randint(964)\n",
    "#     sample_index = np.random.randint(18)\n",
    "    sample_index = np.random.permutation(20)[:2]\n",
    "    \n",
    "    print('sample_p = {},sample_n ={}, sample_index = {}'.format(sample_p,sample_n,sample_index))\n",
    "\n",
    "    posi = np.concatenate(( np.array(dataset_train_positive[sample_p].imgs)  ,  np.array(dataset_negative[sample_n].imgs)[sample_index]   ))\n",
    "    labl =  np.array([dataset_train_positive[sample_p].label, dataset_negative[sample_n].label,dataset_negative[sample_n].label])\n",
    "    print(dataset_train_positive[sample_p].name)\n",
    "    save_path = dataset_train_positive[sample_p].name.split('/home/hyshuai/workspace/dataset/images_evaluation/')[1]\n",
    "    save_path = './generate_img/' + str(save_path)\n",
    "    \n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    print(save_path)\n",
    "    \n",
    "    inputs ,lables = torch.from_numpy(posi).float().to(device),\\\n",
    "                torch.from_numpy(labl).long().to(device)\n",
    "\n",
    "    y_onehot = torch.FloatTensor(3, class_total_num).to(device)\n",
    "    y_onehot.zero_()\n",
    "    y_onehot.scatter_(1, lables.view(-1,1), 1)\n",
    "    # print('y_onehot:', y_onehot)\n",
    "    # z, det, y_logits = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "    z, det = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "    y_logits = net_auto.class_flow(z)\n",
    "    \n",
    "    \n",
    "    obj = z[0] \n",
    "    nn_index = [-1,1]\n",
    "#     print(torch.sum(torch.abs(z[2]),dim =(1,2)))\n",
    "#     print(torch.sum(torch.abs(z[1]),dim =(1,2)))\n",
    "\n",
    "#     print(torch.sum(torch.abs(z[0]),dim =(1,2)))\n",
    "    #         p = np.random.normal(loc=0.5,scale=0.1)\n",
    "    q = np.random.randint(0, 2)\n",
    "    scale = nn_index[q] * p\n",
    "    # print('scale = ', scale)\n",
    "#     start_index = np.random.randint(0,44)\n",
    "    # start_index = 0\n",
    "#     end_index = start_index + 20\n",
    "\n",
    "    slic_index = np.random.permutation(64)[:sampe_index_num]\n",
    "#     print(slic_index) \n",
    "\n",
    "#     print('scale = {},start_index ={}, end_index = {}'.format(scale,start_index,end_index))\n",
    "    # obj[start_index:end_index] = obj[start_index:end_index] + scale * (z[1][start_index:end_index] - z[2][start_index:end_index])\n",
    "    obj[slic_index] = obj[slic_index] + scale * (z[1][slic_index] - z[2][slic_index])\n",
    "\n",
    "    # print(obj.size())\n",
    "    obj = obj.view(-1,*obj.size())\n",
    "    # print(obj.size())\n",
    "    obj_oneshot = y_onehot[0].view(-1,class_total_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_ = net_auto(z = obj,y_onehot = obj_oneshot,eps_std = None ,reverse=True)\n",
    "\n",
    "    x_ = torch.clamp(x_,-1,1)\n",
    "    \n",
    "    diffrent_b = criterion(x_[0] , inputs[0]) / criterion(inputs[0],ones_tensor)\n",
    "    print(' diffrent_b = ',diffrent_b)\n",
    "\n",
    "    c_self = criterion(inputs[0],ones_tensor)\n",
    "    \n",
    "    x_ = np.squeeze(x_.data.cpu().numpy())\n",
    "#     print(x_.shape)\n",
    "    tt = int(round(time.time() * 1000))\n",
    "    misc.imsave(save_path+\"/\"+str(tt)+\"_\"+str(diffrent_b.data.cpu())+ \".png\",x_)\n",
    "#     x_ = cv2.imwrite()\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14,20):\n",
    "\n",
    "    for _ in range(10000):\n",
    "\n",
    "        sample_and_save(i,0.5,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_p = 0 # np.random.randint(20)  # 10 # \n",
    "sample_n = np.random.randint(964)\n",
    "sample_index = np.random.randint(18)\n",
    "print('sample_p = {},sample_n ={}, sample_index = {}'.format(sample_p,sample_n,sample_index))\n",
    "\n",
    "posi = np.concatenate(( np.array(dataset_train_positive[sample_p].imgs)  ,  np.array(dataset_negative[sample_n].imgs[sample_index:sample_index+2])   ))\n",
    "labl =  np.array([dataset_train_positive[sample_p].label, dataset_negative[sample_n].label,dataset_negative[sample_n].label])\n",
    "print(dataset_train_positive[sample_p].name)\n",
    "inputs ,lables = torch.from_numpy(posi).float().to(device),\\\n",
    "            torch.from_numpy(labl).long().to(device)\n",
    "        \n",
    "y_onehot = torch.FloatTensor(3, class_total_num).to(device)\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, lables.view(-1,1), 1)\n",
    "# print('y_onehot:', y_onehot)\n",
    "# z, det, y_logits = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "z, det = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "y_logits = net_auto.class_flow(z)\n",
    "\n",
    "\n",
    "print('z.size() ' ,z.size())\n",
    "print('y_logits.size() ' ,y_logits.size())\n",
    "print(y_logits[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = z[0]\n",
    "# print(obj[0])\n",
    "obj = z[0] \n",
    "nn_index = [-1,1]\n",
    "print(torch.sum(torch.abs(z[2]),dim =(1,2)))\n",
    "print(torch.sum(torch.abs(z[1]),dim =(1,2)))\n",
    "\n",
    "print(torch.sum(torch.abs(z[0]),dim =(1,2)))\n",
    "p =  1.0 # np.random.uniform(0.5, 0.8)\n",
    "#         p = np.random.normal(loc=0.5,scale=0.1)\n",
    "q = np.random.randint(0, 2)\n",
    "scale = nn_index[q] * p\n",
    "# print('scale = ', scale)\n",
    "start_index = np.random.randint(0,44)\n",
    "# start_index = 0\n",
    "end_index = start_index + 20\n",
    "\n",
    "slic_index = np.random.permutation(64)[:20]\n",
    "print(slic_index) \n",
    "\n",
    "print('scale = {},start_index ={}, end_index = {}'.format(scale,start_index,end_index))\n",
    "# obj[start_index:end_index] = obj[start_index:end_index] + scale * (z[1][start_index:end_index] - z[2][start_index:end_index])\n",
    "obj[slic_index] = obj[slic_index] + scale * (z[1][slic_index] - z[2][slic_index])\n",
    "\n",
    "# print(obj.size())\n",
    "obj = obj.view(-1,*obj.size())\n",
    "# print(obj.size())\n",
    "obj_oneshot = y_onehot[0].view(-1,class_total_num)\n",
    "# print(obj_oneshot.size())\n",
    "# print(obj_oneshot)\n",
    "# print(obj[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = z.detach().cpu().numpy()\n",
    "# print(z.size())\n",
    "# print(z[0][0][0])\n",
    "x_ = net_auto(z = obj,y_onehot = obj_oneshot,eps_std = None ,reverse=True)\n",
    "\n",
    "x_ = torch.clamp(x_,-1,1)\n",
    "print(x_.size())\n",
    "\n",
    "print(\"abs sum = \",torch.sum(torch.abs(x_[0] - inputs[0])))\n",
    "# print(\"abs sum = \",torch.sum(x_[0]- inputs[0]))\n",
    "c = criterion(x_[0] , inputs[0])\n",
    "print(\"criterion sum   = \",c)\n",
    "\n",
    "c_self = criterion(inputs[0],ones_tensor)\n",
    "print(\"self criterion sum  = \",c_self)\n",
    "dd =  torch.sum(torch.mul(x_[0],inputs[0]))\n",
    "print(\"  sum  = \",  dd )\n",
    "m1 = torch.norm(x_[0])\n",
    "print(\"  sum  = \",  m1 )\n",
    "m2 = torch.norm(inputs[0])\n",
    "print(\"  sum  = \",  m2 )\n",
    "\n",
    "# l2_self = torch.sum(np.square(x_[0] - inputs[0]))\n",
    "# print(\"l2_self sum  = \",l2_self)\n",
    "\n",
    "print(\"l2  bi li   = \",c / c_self)\n",
    "print(\"cos distance   = \",dd  / (m1 * m2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show(np.squeeze(x_.data))\n",
    "img_show(np.squeeze(inputs.data[0]))\n",
    "imgs_show(np.squeeze(inputs.data),1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_list = [data.imgs for data in dataset_train_positive]\n",
    "train_name_list = [data.name for data in dataset_train_positive]\n",
    "imgs_show(np.squeeze(np.array(train_img_list)),4,5)\n",
    "print(train_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(img):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img,cmap='Greys',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set_style('darkgrid')\n",
    "# sns.set_palette('muted')\n",
    "# sns.set_context(\"notebook\", font_scale=1.5,\n",
    "# rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "label_class = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=501)\n",
    "X = np.squeeze(np.array(train_img_list))\n",
    "\n",
    "X = np.reshape(X,(-1,32 * 32))\n",
    "print(X.shape)\n",
    "result = tsne.fit_transform(X)\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "fig = plot_embedding(result, test_class_index,'t-SNE embedding of the digits (time %.2fs)'% (time() - t0))\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.axis([0, 1000, 0, 1])\n",
    "# plt.ion()\n",
    " \n",
    "# while True:\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for i in range(512):\n",
    "#         x.append(i)\n",
    "#         y.append(np.random.random())\n",
    "#         # plt.pause(0.05)\n",
    "#     plt.cla()\n",
    "#     plt.plot(x, y)\n",
    "#     plt.pause(0.033)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataaaaaaa = get_next_perfect(dataset_train_positive,dataset_negative,8,126)\n",
    "\n",
    "z_list = []\n",
    "labels_list = []\n",
    "for x ,y in dataaaaaaa:\n",
    "    inputs, labels = Variable(torch.from_numpy(x).float()).cuda(), Variable(torch.from_numpy(y).long()).cuda()\n",
    "    print(labels)\n",
    "    z, det = net_auto(x=inputs, y_onehot=None)\n",
    "    z_list.append(z.data.cpu())\n",
    "    labels_list.append(y)\n",
    "print(len(z_list))\n",
    "print(len(labels_list))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_array = np.concatenate(z_list)\n",
    "print(z_array.shape)\n",
    "labels_array = np.concatenate(labels_list)\n",
    "print(labels_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_review = np.reshape(z_array,(-1,64 * 4 * 4))\n",
    "print(z_review.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(data, label, title):\n",
    "    x_min, x_max = np.min(data, 0), np.max(data, 0)\n",
    "    data = (data - x_min) / (x_max - x_min)\n",
    "\n",
    "    fig = plt.figure(figsize=(50,50))\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.text(data[i, 0], data[i, 1], str(label[i]),\n",
    "                 color=plt.cm.Set1(label[i] / 989.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca', random_state=666)\n",
    "\n",
    "result_z = tsne.fit_transform(z_review[:2000])\n",
    "\n",
    "t0 = time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()\n",
    "fig = plot_embedding(result_z, labels_array[:2000],'t-SNE embedding of the digits (time %.2fs)'% (time() - t0))\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
