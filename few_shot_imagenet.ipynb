{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glow import thops\n",
    "from glow import modules_origin\n",
    "from glow import models_origin\n",
    "# from glow import models\n",
    "from glow.config import JsonConfig\n",
    "import cv2\n",
    "from CenterLoss import CenterLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "reload(models_origin)\n",
    "# reload(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# negative\n",
    "path_total = '/home/hyshuai/workspace/dataset/puru'\n",
    "\n",
    "dataset=[]\n",
    "path_list = os.listdir(path_total)\n",
    "path_list.sort()\n",
    "\n",
    "\n",
    "\n",
    "for path_p in path_list:\n",
    "        path_real = os.path.join(path_total,path_p)\n",
    "        class_name = path_real\n",
    "        # image_paths = get_image_paths(path_real)\n",
    "        images = get_images_frompath_random_list_sort_wh(path_real,64,64,600)\n",
    "        print(class_name,\" : \",len(images))\n",
    "        dataset.append(ImageObject(class_name,images))\n",
    "        \n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_negative_pre = []\n",
    "dataset_train_positive =[]\n",
    "dataset_test_positive = []\n",
    "#袋鼠 狮子 河马 浣熊 熊猫\n",
    "\n",
    "for img_obj in dataset:\n",
    "    \n",
    "    if img_obj.name.find('n01877812') == -1 and img_obj.name.find('n02129165') == -1 \\\n",
    "        and img_obj.name.find('n02398521') == -1 and img_obj.name.find('n02509815') == -1 \\\n",
    "        and img_obj.name.find('n02510455') == -1 :\n",
    "        \n",
    "        dataset_negative_pre.append(img_obj)\n",
    "    else:\n",
    "        dataset_train_positive.append(ImageObject(img_obj.name, img_obj.imgs[:5])) # 1-shot\n",
    "        dataset_test_positive.append(ImageObject(img_obj.name, img_obj.imgs[5:]))\n",
    "        print(img_obj.name)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positvie \n",
    "np.random.seed(66)\n",
    "shufindex = np.random.permutation(len(dataset_negative_pre))\n",
    "\n",
    "dataset_negative = np.array(dataset_negative_pre)[shufindex][:80]\n",
    "\n",
    "print(len(dataset_negative[2].imgs),dataset_negative[2].name)\n",
    "\n",
    "\n",
    "print(len(dataset_train_positive[2].imgs),dataset_train_positive[2].name)\n",
    "print(len(dataset_test_positive[2].imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train_positive[1].name)\n",
    "img_show_norm(np.transpose(dataset_train_positive[1].imgs[4],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# label:\n",
    "\n",
    "class_total_num = len(dataset_negative) + len(dataset_train_positive)\n",
    "print(\"class_total = \",class_total_num)\n",
    "np.random.seed(66) \n",
    "shufindex = np.random.permutation(class_total_num)\n",
    "for index, value in enumerate(dataset_negative):\n",
    "    value.label = shufindex[index]\n",
    "    print(value.name,  value.label)\n",
    "\n",
    "for index, value in enumerate(dataset_train_positive):\n",
    "    value.label = shufindex[index+len(dataset_negative)]\n",
    "    dataset_test_positive[index].label = shufindex[index+len(dataset_negative)]\n",
    "    print(value.name, value.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_class_index = [ 79, 77, 51, 60, 20]\n",
    "test_class_index = [data.label for data in dataset_train_positive]\n",
    "print(test_class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_single(dataset_train_positive,dataset_negative,p_size,n_size,shuffle = True):\n",
    "    \"\"\"\n",
    "    final batch_size = p_size + n_size \n",
    "    every dataset_negative should have same imgs leng\n",
    "    \"\"\"\n",
    "    #shuffle\n",
    "    if shuffle:\n",
    "        np.random.shuffle(dataset_train_positive)\n",
    "        np.random.shuffle(dataset_negative)\n",
    "        for data in dataset_negative:\n",
    "            np.random.shuffle(data.imgs)\n",
    "        for data in dataset_train_positive:\n",
    "            np.random.shuffle(data.imgs)\n",
    "    \n",
    "    \n",
    "    batch_negative = n_size \n",
    "#     print('batch_negative ',batch_negative)\n",
    "    len_n_class = len(dataset_negative)\n",
    "#     print('len_n_class ',len_n_class)\n",
    "    len_p_class = len(dataset_train_positive)\n",
    "#     print('len_p_class ',len_p_class)\n",
    "    len_n_imgs = len(dataset_negative[0].imgs)\n",
    "#     print('len_n_imgs ',len_n_imgs)\n",
    "    len_p_imgs = len(dataset_train_positive[0].imgs)\n",
    "    \n",
    "    negative_number = 0\n",
    "    \n",
    "    \n",
    "#     shuffindex = np.random.permutation(len_n)\n",
    "# #     print(shuffindex)\n",
    "#     np.random.shuffle(dataset_train_positive)\n",
    "#     np.random.shuffle(dataset_negative)\n",
    "    \n",
    "    for data in dataset_negative:\n",
    "        negative_number += len(data.imgs)\n",
    "#         np.random.shuffle(data.imgs)\n",
    "#     print('negative_number ',negative_number)\n",
    "    batch_total = negative_number  // batch_negative\n",
    "#     print('batch_total ',batch_total)\n",
    "    i_n_class = 0\n",
    "    i_n_img = 0\n",
    "    i_p_class = 0\n",
    "    i_p_img = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    for b in range(batch_total):\n",
    "        train_x =[]\n",
    "        train_y = []\n",
    "        create_negative_imgs = 0\n",
    "        while create_negative_imgs <= (batch_total * batch_negative) and create_negative_imgs < batch_negative:\n",
    "            \n",
    "            # while #如果一次循环没够，需要再次循环\n",
    "            if i_n_class >= len_n_class:\n",
    "                    i_n_class = 0\n",
    "                    i_n_img +=1\n",
    "                    if (i_n_img+1)  >len_n_imgs:\n",
    "                        i_n_img = 0\n",
    "#                     print('if i_n_class >= len_n_class i_n_class ',i_n_class)\n",
    "#                     print('if i_n_class >= len_n_class i_n_img ',i_n_img)\n",
    "            \n",
    "            for i in range(i_n_class,len_n_class):\n",
    "                    \n",
    "#                 print('for i in range(i_n_class i_n_class ',i_n_class)\n",
    "#                 print('for i in range(i_n_class i_n_img ',i_n_img)\n",
    "                train_x += dataset_negative[i_n_class].imgs[i_n_img  : (i_n_img+1) ]\n",
    "                train_y += [dataset_negative[i_n_class].label] \n",
    "                \n",
    "                i_n_class+=1\n",
    "                if i_n_class >= len_n_class:\n",
    "                    i_n_class = 0\n",
    "                    i_n_img +=1\n",
    "                    if (i_n_img+1)  > len_n_imgs:\n",
    "                        i_n_img = 0\n",
    "#                     print('if i_n_class >= len_n_class i_n_class ',i_n_class)\n",
    "#                     print('if i_n_class >= len_n_class i_n_img ',i_n_img)\n",
    "            \n",
    "                create_negative_imgs +=1\n",
    "                if create_negative_imgs >= batch_negative:\n",
    "#                     print(\"create_negative_imgs ok \",create_negative_imgs)\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "        create_positive_imgs = 0    \n",
    "        while create_positive_imgs < p_size:\n",
    "            for i in range(i_p_class,len_p_class):\n",
    "#                 print('for i in range(i_p_class     ',i_p_class)\n",
    "#                 print('for i in range(i_p_img  ',i_p_img)\n",
    "                \n",
    "                train_x += dataset_train_positive[i_p_class].imgs[i_p_img:(i_p_img +1)]\n",
    "                train_y += [dataset_train_positive[i_p_class].label]\n",
    "                \n",
    "                i_p_class+=1\n",
    "                if i_p_class >= len_p_class:\n",
    "                    i_p_class = 0\n",
    "                    i_p_img +=1\n",
    "                    if (i_p_img +1) > len_p_imgs:\n",
    "                        i_p_img = 0\n",
    "#                     print('if i_p_class >= len_p_class i_p_class ',i_p_class)\n",
    "#                     print('if i_p_class >= len_p_class i_p_img ',i_p_img)\n",
    "                \n",
    "                create_positive_imgs+=1\n",
    "                if create_positive_imgs >= p_size:\n",
    "#                     print(\"create_positive_imgs ok \",create_positive_imgs)\n",
    "                    break\n",
    "        \n",
    "        shuffindex_out = np.random.permutation(len(train_y))\n",
    "#         print(train_x[0].shape)\n",
    "        train_x = np.stack(train_x,axis = 0)\n",
    "#         print('after concat : ',train_x[0].shape)\n",
    "        train_x = train_x[shuffindex_out]\n",
    "        train_y = np.array(train_y)[shuffindex_out]\n",
    "        \n",
    "        yield train_x,train_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_perfect(dataset_train_positive,dataset_negative,p_size,n_size,shuffle = True):\n",
    "    \"\"\"\n",
    "    final batch_size = p_size + n_size * 2\n",
    "    every dataset_negative should have same imgs leng\n",
    "    \"\"\"\n",
    "    #shuffle\n",
    "    if shuffle:\n",
    "        np.random.shuffle(dataset_train_positive)\n",
    "        np.random.shuffle(dataset_negative)\n",
    "        for data in dataset_negative:\n",
    "            np.random.shuffle(data.imgs)\n",
    "        for data in dataset_train_positive:\n",
    "            np.random.shuffle(data.imgs)\n",
    "    \n",
    "    \n",
    "    batch_negative = n_size * 2\n",
    "#     print('batch_negative ',batch_negative)\n",
    "    len_n_class = len(dataset_negative)\n",
    "#     print('len_n_class ',len_n_class)\n",
    "    len_p_class = len(dataset_train_positive)\n",
    "#     print('len_p_class ',len_p_class)\n",
    "    len_n_imgs = len(dataset_negative[0].imgs)\n",
    "#     print('len_n_imgs ',len_n_imgs)\n",
    "    len_p_imgs = len(dataset_train_positive[0].imgs)\n",
    "    \n",
    "    negative_number = 0\n",
    "    \n",
    "    \n",
    "#     shuffindex = np.random.permutation(len_n)\n",
    "# #     print(shuffindex)\n",
    "#     np.random.shuffle(dataset_train_positive)\n",
    "#     np.random.shuffle(dataset_negative)\n",
    "    \n",
    "    for data in dataset_negative:\n",
    "        negative_number += len(data.imgs)\n",
    "#         np.random.shuffle(data.imgs)\n",
    "#     print('negative_number ',negative_number)\n",
    "    batch_total = negative_number  // batch_negative\n",
    "#     print('batch_total ',batch_total)\n",
    "    i_n_class = 0\n",
    "    i_n_img = 0\n",
    "    i_p_class = 0\n",
    "    i_p_img = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    for b in range(batch_total):\n",
    "        train_x =[]\n",
    "        train_y = []\n",
    "        create_negative_imgs = 0\n",
    "        while create_negative_imgs <= (batch_total * batch_negative) and create_negative_imgs < batch_negative:\n",
    "            \n",
    "            # while #如果一次循环没够，需要再次循环\n",
    "            if i_n_class >= len_n_class:\n",
    "                    i_n_class = 0\n",
    "                    i_n_img +=1\n",
    "                    if (i_n_img+1) *2 >len_n_imgs:\n",
    "                        i_n_img = 0\n",
    "#                     print('if i_n_class >= len_n_class i_n_class ',i_n_class)\n",
    "#                     print('if i_n_class >= len_n_class i_n_img ',i_n_img)\n",
    "            \n",
    "            for i in range(i_n_class,len_n_class):\n",
    "                    \n",
    "#                 print('for i in range(i_n_class i_n_class ',i_n_class)\n",
    "#                 print('for i in range(i_n_class i_n_img ',i_n_img)\n",
    "                train_x += dataset_negative[i_n_class].imgs[i_n_img * 2 : (i_n_img+1) *2]\n",
    "                train_y += [dataset_negative[i_n_class].label] * 2\n",
    "                \n",
    "                i_n_class+=1\n",
    "                if i_n_class >= len_n_class:\n",
    "                    i_n_class = 0\n",
    "                    i_n_img +=1\n",
    "                    if (i_n_img+1) *2 > len_n_imgs:\n",
    "                        i_n_img = 0\n",
    "#                     print('if i_n_class >= len_n_class i_n_class ',i_n_class)\n",
    "#                     print('if i_n_class >= len_n_class i_n_img ',i_n_img)\n",
    "            \n",
    "                create_negative_imgs +=2\n",
    "                if create_negative_imgs >= batch_negative:\n",
    "#                     print(\"create_negative_imgs ok \",create_negative_imgs)\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "        create_positive_imgs = 0    \n",
    "        while create_positive_imgs < p_size:\n",
    "            for i in range(i_p_class,len_p_class):\n",
    "#                 print('for i in range(i_p_class     ',i_p_class)\n",
    "#                 print('for i in range(i_p_img  ',i_p_img)\n",
    "                \n",
    "                train_x += dataset_train_positive[i_p_class].imgs[i_p_img:(i_p_img +1)]\n",
    "                train_y += [dataset_train_positive[i_p_class].label]\n",
    "                \n",
    "                i_p_class+=1\n",
    "                if i_p_class >= len_p_class:\n",
    "                    i_p_class = 0\n",
    "                    i_p_img +=1\n",
    "                    if (i_p_img +1) > len_p_imgs:\n",
    "                        i_p_img = 0\n",
    "#                     print('if i_p_class >= len_p_class i_p_class ',i_p_class)\n",
    "#                     print('if i_p_class >= len_p_class i_p_img ',i_p_img)\n",
    "                \n",
    "                create_positive_imgs+=1\n",
    "                if create_positive_imgs >= p_size:\n",
    "#                     print(\"create_positive_imgs ok \",create_positive_imgs)\n",
    "                    break\n",
    "        \n",
    "        shuffindex_out = np.random.permutation(len(train_y))\n",
    "#         print(train_x[0].shape)\n",
    "        train_x = np.stack(train_x,axis = 0)\n",
    "#         print('after concat : ',train_x[0].shape)\n",
    "        train_x = train_x[shuffindex_out]\n",
    "        train_y = np.array(train_y)[shuffindex_out]\n",
    "        \n",
    "        yield train_x,train_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataaaaaaa = get_next_single(dataset_train_positive,dataset_negative,1,11)\n",
    "# # x,y = dataaaaaaa.next()\n",
    "# # print('x o shape ' ,x.shape,y.shape)\n",
    "# for x ,y in dataaaaaaa:\n",
    "#     print(y)\n",
    "#     print('x o shape ' ,x.shape,y.shape)\n",
    "#     print(\"one batch is over\")\n",
    "dataaaaaaa =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnext_test(dataset_test_positive,test_class_index):\n",
    "    \n",
    "    for data in dataset_test_positive:\n",
    "        x = np.stack(data.imgs,axis =0)\n",
    "        y = np.array([test_class_index.index(data.label)] * len(data.imgs))\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataaaaaaa = getnext_test(dataset_test_positive,test_class_index)\n",
    "# for x ,y in dataaaaaaa:\n",
    "#     print(y)\n",
    "#     print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_show_norm(imgs,row,col):\n",
    "    fig,ax = plt.subplots(nrows=row,ncols=col,sharex=True,sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    for i in range(row*col):\n",
    "        img = imgs[i]\n",
    "        min = np.min(img)\n",
    "#         print('min = ',min)\n",
    "        img = np.subtract(img,min) # 0->\n",
    "        max = np.max(img)\n",
    "#         print('max =',max)\n",
    "        img = np.divide(img,max)\n",
    "        \n",
    "        ax[i].imshow(img,cmap='Greys', interpolation='nearest')\n",
    "    \n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_norm(img,size=1):\n",
    "    min_ = np.min(img)\n",
    "#     print('min = ',min_)\n",
    "    img = np.subtract(img,min_) # 0->\n",
    "    max_ = np.amax(a = img,keepdims=False)\n",
    "#     print('max =',max_)\n",
    "    img = np.divide(img,max_)  # 0->1\n",
    "#     print(img)\n",
    "    plt.figure(figsize=(size,size))\n",
    "    plt.imshow(img,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(img):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img,interpolation='nearest')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_show(imgs,row,col):\n",
    "    fig,ax = plt.subplots(nrows=row,ncols=col,sharex=True,sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    for i in range(row*col):\n",
    "        img = imgs[i]\n",
    "        \n",
    "        ax[i].imshow(img,cmap='Greys', interpolation='nearest')\n",
    "    \n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_index_logical_equal(inputs,indexs):\n",
    "    logic_result = (inputs==indexs[0])\n",
    "    for value in indexs:\n",
    "        logic_result = np.logical_or(inputs == value,logic_result)\n",
    "    return logic_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_digits = class_total_num\n",
    "net_auto = models_origin.Glow(JsonConfig(\"hparams/celeba_imagenet.json\"),L=4,K=32)\n",
    "\n",
    "print(net_auto)\n",
    "# centerloss = CenterLoss(class_total_num, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# net_auto.load_state_dict(torch.load('model_glow_imagenet_32k_5way_5shot_add_split_z_is_mean_10.pt'),strict=True)\n",
    "# centerloss.load_state_dict(torch.load('model_center_imagenet_16k_5way_5shot_add_split_z_is_mean_center_0_02_8.pt'),strict=True)\n",
    "# for s in net_auto.flow.output_shapes:\n",
    "#     print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(666)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "net_auto.to(device)\n",
    "net_auto.float()\n",
    "# centerloss.to(device)\n",
    "# centerloss.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "center_loss_weight = 0.02\n",
    "optimizer = optim.Adam(net_auto.parameters(),lr=0.001) # decoder don't weight_decay\n",
    "# optimzer4center = optim.SGD(centerloss.parameters(), lr =0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train_nor\n",
    "# net_auto.set_z_add_random(True)\n",
    "batch_size = 16\n",
    "net_auto.train()\n",
    "for epoch in range(30001):\n",
    "    #train:\n",
    "    \n",
    "    datas =  get_next_single(dataset_train_positive,dataset_negative,0,16,shuffle=True)\n",
    "#     datas = getnext(dataset_train_positive,dataset_negative)\n",
    "    \n",
    "    trainloss_g = 0\n",
    "    trainloss_c = 0\n",
    "    \n",
    "    for x,y in datas:\n",
    "        inputs ,lables = torch.from_numpy(x).float().to(device),\\\n",
    "            torch.from_numpy(y).long().to(device)\n",
    "#         print(torch.sum(lables)) # = 1167\n",
    "        \n",
    "        y_onehot = torch.FloatTensor(batch_size, class_total_num).to(device)\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, lables.view(-1,1), 1)\n",
    "#         print('y_onehot:', y_onehot)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         optimzer4center.zero_grad()\n",
    "        \n",
    "        z, det, y_logits= net_auto(x=inputs, y_onehot=y_onehot)\n",
    "#         y_logits = net_auto.class_flow(z)\n",
    "#         print(\"out size = \",z.size())\n",
    "        \n",
    "        loss_g = models_origin.Glow.loss_generative(det)\n",
    "        loss_c = models_origin.Glow.loss_class(y_logits,lables)\n",
    "#         loss_center = centerloss(lables, z.mean(2).mean(2))\n",
    "        \n",
    "#         print('%d  loss_g: %.4f, loss_c:%.4f ,loss_center: %.4f' % (epoch , loss_g.item(),loss_c.item(),loss_center.item()))\n",
    "        print('%d  loss_g: %.4f, loss_c:%.4f' % (epoch , loss_g.item(),loss_c.item()))\n",
    "        loss = loss_g + loss_c * 0.01 #+ center_loss_weight * loss_center\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(net_auto.parameters(), 5)\n",
    "        torch.nn.utils.clip_grad_norm_(net_auto.parameters(), 100)\n",
    "        \n",
    "        \n",
    "        trainloss_g += loss_g.item()\n",
    "        trainloss_c += loss_c.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "#         optimzer4center.step()\n",
    "        \n",
    "    print('%d  gloss: %.3f, class loss:%.3f' % (epoch ,  trainloss_g,trainloss_c))\n",
    "    \n",
    "    if epoch % 2 == 0 and epoch >0:\n",
    "        \n",
    "        torch.save(net_auto.state_dict(),'model_glow_imagenet_32k_5way_5shot_add_split_z_is_mean_no_p_'+str(epoch)+'.pt')\n",
    "#         torch.save(centerloss.state_dict(),'model_center_imagenet_16k_5way_5shot_add_split_z_is_mean_center_0_02_'+str(epoch)+'.pt')\n",
    "#24  loss_g: 3.3515, loss_c:2.6742\n",
    "#24  loss_g: 3.3070, loss_c:2.8227\n",
    "#24  loss_g: 3.3131, loss_c:2.9545    \n",
    "# 35  loss_g: 3.5125, loss_c:1.9239\n",
    "#29  loss_g: 3.4648, loss_c:2.5662\n",
    "#29  loss_g: 3.3188, loss_c:2.9107\n",
    "#29  loss_g: 3.4770, loss_c:1.8553\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_auto.set_z_add_random(True)\n",
    "# net_auto.train()\n",
    "epoch = 78\n",
    "# torch.save(net_auto.state_dict(),'model_glow_imagenet_32k_5way_5shot_add_split_z_is_mean_'+str(epoch)+'.pt')\n",
    "# torch.save(centerloss.state_dict(),'model_center_imagenet_16k_5way_5shot_add_split_z_is_mean_center_0_02_11.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_auto.set_z_add_random(False)\n",
    "sample_p =  np.random.randint(5)  # 10 # \n",
    "sample_p_index =  np.random.randint(5)\n",
    "sample_n =  np.random.randint(80)       # 669 # 292 #   500 #    669 # |\n",
    "sample_index = np.random.randint(600)    # 13  # 1 #    # 13   #\n",
    "print('sample_p = {},sample_n ={}, sample_index = {}'.format(sample_p,sample_n,sample_index))\n",
    "\n",
    "posi = np.concatenate(( np.array(dataset_train_positive[sample_p].imgs[sample_p_index:sample_p_index+1]) , \\\n",
    "                       np.array(dataset_negative[sample_n].imgs[sample_index:sample_index+2])))\n",
    "labl =  np.array([dataset_train_positive[sample_p].label, dataset_negative[sample_n].label,dataset_negative[sample_n].label])\n",
    "print(dataset_train_positive[sample_p].name)\n",
    "print(dataset_negative[sample_n].name)\n",
    "print(labl)\n",
    "inputs ,lables = torch.from_numpy(posi).float().to(device),\\\n",
    "            torch.from_numpy(labl).long().to(device)\n",
    "        \n",
    "y_onehot = torch.FloatTensor(3, class_total_num).to(device)\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, lables.view(-1,1), 1)\n",
    "# print('y_onehot:', y_onehot)\n",
    "# z, det, y_logits = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "z, det,y_lo = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "# y_logits = net_auto.class_flow(z)\n",
    "\n",
    "\n",
    "obj = z[0] #+ 0.5 *(z[1] - z[2])\n",
    "obj = obj.view(-1,*obj.size())\n",
    "obj_oneshot = y_onehot[0].view(-1,class_total_num)\n",
    "x_ = net_auto(z = obj,y_onehot = obj_oneshot,eps_std = 1e-8 ,reverse=True)\n",
    "x_ = torch.clamp(x_,-1,1)\n",
    "print(x_.size())\n",
    "\n",
    "print(\"abs sum = \",torch.sum(torch.abs(x_[0] - inputs[0])))\n",
    "# c = criterion(x_[0] , inputs[0])\n",
    "# print(\"criterion sum   = \",c)\n",
    "# c_self = criterion(inputs[0],ones_tensor)\n",
    "# print(\"self criterion sum  = \",c_self)\n",
    "# print(\"divider square = \", torch.sum(torch.mul(divider_z,divider_z)))\n",
    "# print(\"l2  bi li   = \",c / c_self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_.data.size())\n",
    "# img_show_norm(np.transpose(dataset_negative[1].imgs[5],(1,2,0)))\n",
    "# img_show_norm(np.transpose(np.squeeze( x_.data.cpu().numpy()),(1,2,0)))\n",
    "# img_show_norm(np.transpose(np.squeeze( inputs.data[0].cpu().numpy()),(1,2,0)))\n",
    "# img_show_norm(np.transpose(np.squeeze( inputs.data[1].cpu().numpy()),(1,2,0)))\n",
    "# img_show_norm(np.transpose(np.squeeze( inputs.data[2].cpu().numpy()),(1,2,0)))\n",
    "x_ = None\n",
    "inputs = None\n",
    "z = None\n",
    "det =None\n",
    "y_lo = None\n",
    "obj = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show_norm(np.transpose(np.squeeze( x_.data.cpu().numpy()),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_auto.load_state_dict(torch.load('model_glow_ominst_all_bg_8k_20_way_5_shot_160.pt'))\n",
    "net_auto.set_z_add_random(False)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(666)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "net_auto.to(device)\n",
    "net_auto.float()\n",
    "net_auto.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "test_datas = getnext_test(dataset_test_positive,test_class_index)\n",
    "for x ,y in test_datas:\n",
    "        inputs, labels = Variable(torch.from_numpy(x).float()).cuda(), Variable(torch.from_numpy(y).long()).cuda()\n",
    "#         print(labels)\n",
    "        \n",
    "        z, det = net_auto(x=inputs, y_onehot=None)\n",
    "        classify = net_auto.class_flow(z)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         _,classify = net_auto(inputs)\n",
    "        test_loss += F.nll_loss(classify, labels, size_average=False).item() # sum up batch loss\n",
    "#         print(classify)\n",
    "        pred = classify.max(1,keepdim=True)[1]\n",
    "        print(pred)\n",
    "        \n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "#         print(correct)\n",
    "print(correct)\n",
    "print(correct * 1.0 / (19 * 20))\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(x_p.shape)\n",
    "print(label_positive)\n",
    "imgs_show(np.squeeze(x_positive),4,5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_show(np.squeeze(images_observe_train_total[80:100]),4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_show(np.squeeze(images_observe_train_total[0:20]),4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_mean_var():\n",
    "    batch_size = 256\n",
    "    net_auto.set_z_add_random(False)\n",
    "    net_auto.eval()\n",
    "    mean_std_list =[MeanVarObject(i) for i in range(class_total_num)]\n",
    "    \n",
    "    datas =  get_next_perfect(dataset_train_positive,dataset_negative,6,125,shuffle=False)\n",
    "    \n",
    "    for x,y in datas:\n",
    "        inputs ,lables = torch.from_numpy(x).float().to(device),\\\n",
    "            torch.from_numpy(y).long().to(device)\n",
    "#         print(torch.sum(lables)) # = 1167\n",
    "        \n",
    "        y_onehot = torch.FloatTensor(batch_size, class_total_num).to(device)\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, lables.view(-1,1), 1)\n",
    "#         print('y_onehot:', y_onehot)\n",
    "        optimizer.zero_grad()\n",
    "        optimzer4center.zero_grad()\n",
    "        \n",
    "        z , _ = net_auto(x=inputs, y_onehot=y_onehot)\n",
    "        z_real = z.detach().cpu().numpy()\n",
    "        for i,label in enumerate(y) :\n",
    "            mean_std_list[label].add_z(z_real[i])\n",
    "    return mean_std_list\n",
    "\n",
    "mean_std_obj_list = get_real_mean_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_std_list label : 0 ->984\n",
    "\n",
    "\n",
    "mean_list = []\n",
    "std_list =[]\n",
    "z_total_list = []\n",
    "\n",
    "for obj in mean_std_obj_list:\n",
    "#     print(obj.label)\n",
    "    mean_list.append(obj.get_mean())\n",
    "    std_list.append(obj.get_std())\n",
    "    z_total_list.append(obj.get_z_array())\n",
    "\n",
    "mean_list  = np.array(mean_list)\n",
    "std_list = np.array(std_list)\n",
    "z_total_list = np.concatenate(np.array(z_total_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(z_total_list.shape)\n",
    "mean_z = np.mean(z_total_list,axis=0)\n",
    "std_z = np.std(z_total_list,axis=0)\n",
    "color_z = z_total_list * 250\n",
    "# print(mean_std_obj_list[500].get_mean()[2,:,:])\n",
    "# print(mean_list.shape)\n",
    "# print(mean_list[500,2,:,:])\n",
    "# # print(mean_x[500,2,:,:])\n",
    "# # print(mean_x[20,2,:,:])\n",
    "# print(mean_std_obj_list[500].get_std()[2,:,:])\n",
    "# print(std_list[500,2,:,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_u_total(z_list,mean_z):\n",
    "    # mean_std_obj_list[381].get_z_array()\n",
    "    divider = np.subtract(z_list,mean_z) ** 2\n",
    "#     print(divider.shape)\n",
    "    return np.mean(divider,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(2000,1),dpi=30)\n",
    "# # plt.text(17527,100,'Pre')\n",
    "# # my_y_ticks = np.arange(-0.1, 0.1, 0.01)\n",
    "# # plt.yticks(my_y_ticks)\n",
    "# d = 2\n",
    "# w = 1\n",
    "# h = 0\n",
    "# print('std_z = ',std_z[d,w,h])\n",
    "# print('mean = ',u_total_real[d,w,h])\n",
    "\n",
    "# plt.scatter(x = z_total_list[:,d,w,h],y = np.ones_like(z_total_list[:,d,w,h]),s=75,c =color_z[:,d,w,h],  alpha=.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('std_z = ',std_z[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = mean_list * 250\n",
    "u_total_real = np.mean(mean_list,axis=0)\n",
    "std_mean_real  = np.mean(np.square(std_list),axis=0)\n",
    "ui_squ_mean_real  = np.mean(np.square(mean_list),axis=0)\n",
    "u_total_sque_real = np.square(u_total_real)\n",
    "std_squ_total = std_mean_real + ui_squ_mean_real - u_total_sque_real\n",
    "std_total_real = np.sqrt(std_squ_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean = ',u_total_real[2,:,:])\n",
    "print(\"  \",mean_list[20,2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('u_total =  ',u_total_real[2,:,:])\n",
    "# print('std_mean =  ',np.sqrt(std_mean_real[:,0,0]))\n",
    "# print('ui_squ_mean =  ',ui_squ_mean.shape)\n",
    "# print('u_total_sque =  ',u_total_sque.shape)\n",
    "# print('std_squ_total =  ',std_squ_total.shape)\n",
    "d = 2\n",
    "w = 1\n",
    "h = 3\n",
    "print('std_z = ',std_z[d,w,h])\n",
    "print('mean = ',u_total_real[d,w,h])\n",
    "print('std_list = ',std_list[100:200,d,w,h])\n",
    "# print('mean list                = \\n',mean_list[25:80,d,w,h])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(300,2))\n",
    "\n",
    "# my_y_ticks = np.arange(-0.1, 0.1, 0.01)\n",
    "# plt.yticks(my_y_ticks)\n",
    "\n",
    "\n",
    "print('std = ',std_total_real[d,w,h])\n",
    "print('mean = ',u_total_real[d,w,h])\n",
    "plt.scatter(x = mean_list[:,d,w,h],y = np.ones_like(mean_list[:,d,w,h]),s=75,c =color[:,d,w,h],  alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables_obser_bias = [241 ,188, 635, 270, 774, 790, 772, 376, 201 , 26, 453, 785 ,875 ,634 ,527 ,718 ,299 ,  6 ,679, 373]\n",
    "for index ,value in enumerate(lables_obser_bias):\n",
    "    print(str(index+1),\" scale = \",np.sum(np.sort(np.reshape(std_list[value,:,:,:],64*4*4))[-50:]) / np.sum(std_list[value,:,:,:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables_obser_bias = [543, 119 ,482 ,541, 806, 602, 211, 757, 600, 969, 942, 107, 752, 729, 698, 153 ,466 ,597,578 ,604]\n",
    "for index ,value in enumerate(lables_obser_bias):\n",
    "    print(str(index+1),\" scale = \",np.sum(np.sort(np.reshape(std_list[value,:,:,:],64*4*4))[-50:]) / np.sum(std_list[value,:,:,:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables_obser_bias = [344 ,539, 446, 566 ,431, 680, 161, 164, 336 ,980, 132 , 38, 169 ,660, 795 ,286 ,337 ,939,710, 973]\n",
    "for index ,value in enumerate(lables_obser_bias):\n",
    "    print(str(index+1),\" scale = \",np.sum(np.sort(np.reshape(std_list[value,:,:,:],64*4*4))[-50:]) / np.sum(std_list[value,:,:,:]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[847,:,:,:],64*4*4))[-40:]) / np.sum(std_list[847,:,:,:]) )\n",
    "print(\"118 scale = \",np.sum(np.sort(np.reshape(std_list[118,:,:,:],64*4*4))[-40:]) / np.sum(std_list[118,:,:,:]) )\n",
    "print(\"364 scale = \",np.sum(np.sort(np.reshape(std_list[364,:,:,:],64*4*4))[-40:]) / np.sum(std_list[364,:,:,:]) )\n",
    "print(\"239 scale = \",np.sum(np.sort(np.reshape(std_list[239,:,:,:],64*4*4))[-40:]) / np.sum(std_list[239,:,:,:]) )\n",
    "print(\"76 scale = \",np.sum(np.sort(np.reshape(std_list[76,:,:,:],64*4*4))[-40:]) / np.sum(std_list[76,:,:,:]) )\n",
    "print(\"---------------------------------\")\n",
    "print(\"936 scale = \",np.sum(np.sort(np.reshape(std_list[936,:,:,:],64*4*4))[-40:]) / np.sum(std_list[936,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[621,:,:,:],64*4*4))[-40:]) / np.sum(std_list[621,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[275,:,:,:],64*4*4))[-40:]) / np.sum(std_list[275,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[121,:,:,:],64*4*4))[-40:]) / np.sum(std_list[121,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[312,:,:,:],64*4*4))[-40:]) / np.sum(std_list[312,:,:,:]) )\n",
    "print(\"---------------------------------\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[859,:,:,:],64*4*4))[-40:]) / np.sum(std_list[859,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[389,:,:,:],64*4*4))[-40:]) / np.sum(std_list[389,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[536,:,:,:],64*4*4))[-40:]) / np.sum(std_list[536,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[730,:,:,:],64*4*4))[-40:]) / np.sum(std_list[730,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[332,:,:,:],64*4*4))[-40:]) / np.sum(std_list[332,:,:,:]) )\n",
    "print(\"---------------------------------\")\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[760,:,:,:],64*4*4))[-40:]) / np.sum(std_list[760,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[397,:,:,:],64*4*4))[-40:]) / np.sum(std_list[397,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[463,:,:,:],64*4*4))[-40:]) / np.sum(std_list[463,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[750,:,:,:],64*4*4))[-40:]) / np.sum(std_list[750,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[803,:,:,:],64*4*4))[-40:]) / np.sum(std_list[803,:,:,:]) )\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[886,:,:,:],64*4*4))[-40:]) / np.sum(std_list[886,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[882,:,:,:],64*4*4))[-40:]) / np.sum(std_list[882,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[215,:,:,:],64*4*4))[-40:]) / np.sum(std_list[215,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[430,:,:,:],64*4*4))[-40:]) / np.sum(std_list[430,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[861,:,:,:],64*4*4))[-40:]) / np.sum(std_list[861,:,:,:]) )\n",
    "print(\"---------------------------------\")\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[812,:,:,:],64*4*4))[-40:]) / np.sum(std_list[812,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[623,:,:,:],64*4*4))[-40:]) / np.sum(std_list[623,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[381,:,:,:],64*4*4))[-40:]) / np.sum(std_list[381,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[338,:,:,:],64*4*4))[-40:]) / np.sum(std_list[338,:,:,:]) )\n",
    "print(\"847 scale = \",np.sum(np.sort(np.reshape(std_list[840,:,:,:],64*4*4))[-40:]) / np.sum(std_list[840,:,:,:]) )\n",
    "\n",
    "\n",
    "\n",
    "# print(\"std fuza = \",np.sum(std_list[847,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[118,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[364,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[239,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[76,:,:,:]))\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std fuza = \",np.sum(std_list[936,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[621,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[275 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[121 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[312 ,:,:,:]))\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std fuza = \",np.sum(std_list[859,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[389,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[536 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[730 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[332 ,:,:,:]))\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std fuza = \",np.sum(std_list[760,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[397,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[463 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[750 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[803 ,:,:,:]))\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std fuza = \",np.sum(std_list[886,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[882,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[215 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[430 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[861 ,:,:,:]))\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std fuza = \",np.sum(std_list[812,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[623,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[381 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[338 ,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[840 ,:,:,:]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[849,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[5,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[101,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[590,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[235,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[135,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[214,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[465 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[411 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[901 ,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[54,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[977,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[580 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[764 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[403 ,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[519,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[864,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[916 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[675 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[964 ,:,:,:]))\n",
    "\n",
    "\n",
    "print(np.sum(np.sort(np.reshape(std_list[977,:,:,:],64*4*4))[-40:]))\n",
    "print(np.sum(np.sort(np.reshape(std_list[580,:,:,:],64*4*4))[-40:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[220,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[48,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[379,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[294,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[18,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[674,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[97,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[348 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[65 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[831 ,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[547,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[844,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[236 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[172 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[424 ,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[657,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[46,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[3 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[255 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[470 ,:,:,:]))\n",
    "\n",
    "\n",
    "print(np.sum(np.sort(np.reshape(std_list[46,:,:,:],64*4*4))[:-100]))\n",
    "print(np.sum(np.sort(np.reshape(std_list[3,:,:,:],64*4*4))[:-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[220].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[48].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[379].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[294].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[18].get_z_array(),mean_z)))\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print('distance_u_simple  = ',np.sum(compute_distance_u_total(mean_std_obj_list[674].get_z_array(),mean_z)))\n",
    "print('distance_u_simple  = ',np.sum(compute_distance_u_total(mean_std_obj_list[97].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[348].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[65].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[831].get_z_array(),mean_z)))\n",
    "print(\"---------------------------------\")\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[547].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[844].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[236].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[172].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[424].get_z_array(),mean_z)))\n",
    "print(\"---------------------------------\")\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[657].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[46].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[3].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[255].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[470].get_z_array(),mean_z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# distance_u_simple = compute_distance_u_total(mean_std_obj_list[simple].get_z_array(),mean_z)\n",
    "\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[159].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[687].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[486].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[843].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[27].get_z_array(),mean_z)))\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print('distance_u_simple  = ',np.sum(compute_distance_u_total(mean_std_obj_list[253].get_z_array(),mean_z)))\n",
    "print('distance_u_simple  = ',np.sum(compute_distance_u_total(mean_std_obj_list[909].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[295].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[912].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[714].get_z_array(),mean_z)))\n",
    "print(\"---------------------------------\")\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[307].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[661].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[858].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[79].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[717].get_z_array(),mean_z)))\n",
    "print(\"---------------------------------\")\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[122].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[51].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[631].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[828].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[20].get_z_array(),mean_z)))\n",
    "\n",
    "\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std jiandan = \",np.sum(std_list[761,:,:,:]))\n",
    "# print(\"std complex = \",np.sum(std_list[761,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[159,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[687,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[486,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[843,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[27,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[253,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[909,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[295 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[912 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[714 ,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[307,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[661,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[858 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[79 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[717 ,:,:,:]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[122,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[51,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[631 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[828 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[20 ,:,:,:]))\n",
    "\n",
    "\n",
    "print(np.sum(np.sort(np.reshape(std_list[79,:,:,:],64*4*4))[-10:]))\n",
    "print(np.sum(np.sort(np.reshape(std_list[122,:,:,:],64*4*4))[-10:]))\n",
    "print(np.sum(np.sort(np.reshape(std_list[486,:,:,:],64*4*4))[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexs = 239\n",
    "simple = 381\n",
    "print(\"u_total = \",u_total_real[2,:,:])\n",
    "print(\"std_real = \",std_total_real[2,:,:])\n",
    "print(\"---------------------------------\")\n",
    "# print(\"fuza = \",mean_list[239,2,:,:])\n",
    "# print(\"jiandan = \",mean_list[381,2,:,:])\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "# distance_u_complex = compute_distance_u_total(mean_std_obj_list[complexs].get_z_array(),mean_z)\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[847].get_z_array(),mean_z)))\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[118].get_z_array(),mean_z)))\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[239].get_z_array(),mean_z)))\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[76].get_z_array(),mean_z)))\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[936].get_z_array(),mean_z)))\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[621].get_z_array(),mean_z)))\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[891].get_z_array(),mean_z)))\n",
    "print('distance_u_complex = ',np.sum(compute_distance_u_total(mean_std_obj_list[586].get_z_array(),mean_z)))\n",
    "print(\"---------------------------------\")\n",
    "# distance_u_simple = compute_distance_u_total(mean_std_obj_list[simple].get_z_array(),mean_z)\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[886].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[882].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[430].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[812].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[623].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[381].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[579].get_z_array(),mean_z)))\n",
    "print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[559].get_z_array(),mean_z)))\n",
    "# std_fuza = np.sort(np.reshape(std_list[239,:,:,:],16*4*4))\n",
    "# std_jiandan = np.sort(np.reshape(std_list[381,:,:,:],16*4*4))\n",
    "# print(\"std fuza = \",np.sum(std_fuza[100:150]) )\n",
    "# print(\"std jiandan = \",np.sum(std_jiandan[100:150]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"std fuza = \",np.sum(std_list[847,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[118,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[239,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[76,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[936,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[621,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[621,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[366 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[292 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[479 ,:,:,:]))\n",
    "print(\"std fuza = \",np.sum(std_list[955 ,:,:,:]))\n",
    "\n",
    "\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std fuza = \",np.sum(std_list[661,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[858,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[79,:,:,:]))\n",
    "# print(\"std fuza = \",np.sum(std_list[159,:,:,:]))\n",
    "# print(\"---------------------------------\")\n",
    "print(\"std jiandan = \",np.sum(std_list[886,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[882,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[430,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[812,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[623,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[381,:,:,:]))\n",
    "\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"std jiandan = \",np.sum(std_list[761,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[589 ,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[572 ,:,:,:]))\n",
    "print(\"std jiandan = \",np.sum(std_list[129 ,:,:,:]))\n",
    "\n",
    "\n",
    "# print(\"---------------------------------\")\n",
    "# print(\"std jiandan = \",np.sum(std_list[979,:,:,:]))\n",
    "# print('distance_u_simple = ',np.sum(compute_distance_u_total(mean_std_obj_list[979].get_z_array(),mean_z)))\n",
    "\n",
    "# mean_std_obj_list[979].get_z_array()[12:14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = len(label_observe_total)\n",
    "lables = np.array([index for index in range(class_total_num)])\n",
    "lables = torch.from_numpy(lables).long().to(device)\n",
    "print(\"get_log_mean_var ,labels = \", lables)\n",
    "print(lables[-1])\n",
    "y_onehot = torch.FloatTensor(batch_size, class_total_num).cuda()\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, lables.view(-1, 1), 1)\n",
    "\n",
    "mean,var = net_auto.get_log_mean_var(batch_size,y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = torch.exp(var.cpu()).detach().numpy()\n",
    "mean_x = mean.cpu().detach().numpy()\n",
    "# mean_y = np.ones_like(mean_x)\n",
    "color = (mean_x + 1 ) * 150\n",
    "\n",
    "u_total = np.mean(mean_x,axis=0)\n",
    "print('u_total =  ',u_total[:,0,0])\n",
    "std_mean = np.mean(np.square(std),axis=0)\n",
    "print('std_mean =  ',np.sqrt(std_mean[:,0,0]))\n",
    "ui_squ_mean = np.mean(np.square(mean_x),axis=0)\n",
    "print('ui_squ_mean =  ',ui_squ_mean.shape)\n",
    "u_total_sque = np.square(u_total)\n",
    "print('u_total_sque =  ',u_total_sque.shape)\n",
    "std_squ_total = std_mean + ui_squ_mean - u_total_sque\n",
    "std_total = np.sqrt(std_squ_total)\n",
    "print('std_squ_total =  ',std_squ_total.shape)\n",
    "print('std__total = ',std_total[2,:,:])\n",
    "print('std__total = ',std_total_real[2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(200,5))\n",
    "\n",
    "# my_y_ticks = np.arange(-0.1, 0.1, 0.01)\n",
    "# plt.yticks(my_y_ticks)\n",
    "\n",
    "plt.scatter(x = mean_x[:,9,0,0],y = np.ones_like(mean_x[:,9,0,0]),s=75,c =color[:,9,0,0],  alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "ones_tensor = torch.ones((1, 32, 32)).float().cuda()\n",
    "std_total_gpu = torch.from_numpy(std_total_real).cuda()\n",
    "from scipy import misc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = mean_list[122]\n",
    "vec2 = mean_list[470]\n",
    "print(np.sqrt(np.sum(np.square( vec1 - vec2 ))))\n",
    "print(float(np.sum(vec1*vec2)) / np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "print(\"----------------------------------\")\n",
    "vec1 = mean_list[122]\n",
    "vec2 = mean_list[101]\n",
    "print(np.sqrt(np.sum(np.square( vec1 - vec2 ))))\n",
    "print(float(np.sum(vec1*vec2)) / np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "print(\"----------------------------------\")\n",
    "vec1 = mean_list[122]\n",
    "vec2 = mean_list[590]\n",
    "print(np.sqrt(np.sum(np.square( vec1 - vec2 ))))\n",
    "print(float(np.sum(vec1*vec2)) / np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "ones_tensor = torch.ones((3, 128, 128)).float().cuda()\n",
    "from scipy import misc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sqrt(np.sum(np.square( mean_div ))))\n",
    "# mean_bi = np.clip( abs(mean_div / (std_total_real )),0,1)\n",
    "# print(mean_bi[0:5])\n",
    "\n",
    "\n",
    "# # mean_scale  = torch.from_numpy(np.cos(mean_bi)).cuda()\n",
    "# mean_scale  = torch.reciprocal(torch.exp(torch.from_numpy(mean_bi).cuda()))\n",
    "# print(mean_scale[0:5])\n",
    "# nagi_div_bi_std = abs(z[2] - z[1]) / torch.from_numpy(std_list[lable_n]).cuda()\n",
    "# print(nagi_div_bi_std[0:5])\n",
    "# n_jian_chu_std = torch.clamp(2 / (nagi_div_bi_std + 1e-8),0,1.5)\n",
    "# print(n_jian_chu_std[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(z[0,0,:,:])\n",
    "# print(z[1,0,:,:])\n",
    "# print(z[2,0,:,:])\n",
    "# divi = z[1]-z[0]\n",
    "# print(divi[0])\n",
    "\n",
    "# std_scale = 1.0 / np.exp(abs(mean_list[labl[0]] - mean_list[labl[1]]) / std_total_real ) * 0.5\n",
    "# print(std_scale) std_total_gpu 可能非常小，造成divide非常大\n",
    "# divide = torch.div(torch.abs((z[1]- z[0])), std_total_gpu * 0.5 ) \n",
    "# # scale = torch.cos(torch.clamp(divide,0,1.57))\n",
    "\n",
    "# scale = torch.reciprocal(torch.exp(divide))* 0.4 \n",
    "\n",
    "# # scale = torch.abs(torch.div(std_total_gpu * 0.3, torch.add(torch.sub(z[1],z[0]),1e-4)))\n",
    "# # scale = torch.clamp(scale,0,1)\n",
    "# print(scale)\n",
    "# # scale = 0.39\n",
    "# divider_z = scale * (z[2] - z[1]) \n",
    "\n",
    "# obj = z[0]+ divider_z\n",
    "\n",
    "divide = torch.div(torch.abs(torch.sub(z[1],z[0])), std_total_gpu ) \n",
    "# scale = torch.cos(torch.clamp(divide,0,1.57))\n",
    "\n",
    "scale = torch.reciprocal(torch.exp(divide)) \n",
    "# scale = torch.abs(torch.div(std_total_gpu * 0.3, torch.add(torch.sub(z[1],z[0]),1e-4)))\n",
    "# scale = torch.clamp(scale,0,1)\n",
    "# print(scale > 1e-1)\n",
    "# scale = 0.39\n",
    "divider_z = scale * (z[2] - z[1])\n",
    "obj = z[0]+ divider_z\n",
    "\n",
    "\n",
    "# obj = z[0]\n",
    "# obj[0:30] = obj[0:30]+ divider_z[0:30]\n",
    "# d = 0\n",
    "# w = 0\n",
    "# h = 0\n",
    "# print('obj after divider = ' ,obj[d,w,h])\n",
    "# obj[d,w,h] = obj[d,w,h] + 50 * (z[2,d,w,h] - z[1,d,w,h])\n",
    "# print('obj after compute mean = ' ,obj[d,w,h])\n",
    "\n",
    "\n",
    "\n",
    "# print('obj mean = ',mean_x[-7,:,0,0])\n",
    "# print('std_squ_total = ',std_squ_total[:,0,0])\n",
    "\n",
    "# print(obj.size())\n",
    "obj = obj.view(-1,*obj.size())\n",
    "# print(obj.size())\n",
    "obj_oneshot = y_onehot[0].view(-1,class_total_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = net_auto(z = obj,y_onehot = obj_oneshot,eps_std = 1e-8 ,reverse=True)\n",
    "\n",
    "x_ = torch.clamp(x_,-1,1)\n",
    "print(x_.size())\n",
    "\n",
    "print(\"abs sum = \",torch.sum(torch.abs(x_[0] - inputs[0])))\n",
    "# print(\"abs sum = \",torch.sum(x_[0]- inputs[0]))\n",
    "c = criterion(x_[0] , inputs[0])\n",
    "print(\"criterion sum   = \",c)\n",
    "\n",
    "c_self = criterion(inputs[0],ones_tensor)\n",
    "print(\"self criterion sum  = \",c_self)\n",
    "# dd =  torch.sum(torch.mul(x_[0],inputs[0]))\n",
    "# print(\"  sum  = \",  dd )\n",
    "# m1 = torch.norm(x_[0])\n",
    "# print(\"  sum  = \",  m1 )\n",
    "# m2 = torch.norm(inputs[0])\n",
    "# print(\"  sum  = \",  m2 )\n",
    "\n",
    "# l2_self = torch.sum(np.square(x_[0] - inputs[0]))\n",
    "# print(\"l2_self sum  = \",l2_self)\n",
    "print(\"divider square = \", torch.sum(torch.mul(divider_z,divider_z)))\n",
    "print(\"l2  bi li   = \",c / c_self)\n",
    "# print(\"cos distance   = \",dd  / (m1 * m2) )\n",
    "print(str(sample_n),\" scale = \",np.sum(np.sort(np.reshape(std_list[sample_n,:,:,:],64*4*4))[-50:]) / np.sum(std_list[sample_n,:,:,:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show(np.squeeze(x_.data))\n",
    "img_show(np.squeeze(inputs.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_show(np.squeeze(np.concatenate((x_.data,inputs.data)) ),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_show(np.squeeze(np.concatenate((x_.data,inputs.data)) ),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_show(np.squeeze(np.concatenate((x_.data,inputs.data)) ),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_show(np.squeeze(np.concatenate((x_.data,inputs.data)) ),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_i = 10\n",
    "print('mean_x -7 = ',mean_x[sample_p,:dim_i,0,0])\n",
    "print('mean_x 100 = ',mean_x[100,:dim_i,0,0])\n",
    "print('std -7 = ',std[sample_p,:dim_i,0,0])\n",
    "print('std 100 = ',std[100,:dim_i,0,0])\n",
    "\n",
    "\n",
    "print('u_total= ',u_total[:dim_i,0,0])\n",
    "distance = mean_x[-7,:dim_i,0,0] -u_total[:dim_i,0,0]\n",
    "\n",
    "print('sample mean distance mean_total = ', distance)\n",
    "print('max =',np.max(distance))\n",
    "print('min =',np.min(distance))\n",
    "\n",
    "print('std__total = ',std_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = z[0]\n",
    "\n",
    "scale = 1.0\n",
    "# print('scale = ', scale)\n",
    "start_index = np.random.randint(0,44)\n",
    "# start_index = 0\n",
    "end_index = start_index + 20\n",
    "print('scale = {},start_index ={}, end_index = {}'.format(scale,start_index,end_index))\n",
    "# print('----------------------2-------------------------------')\n",
    "# max_std_index = torch.argmax(std_[-7,start_index:end_index,0,0])\n",
    "# print(max_std_index)\n",
    "# print(std_[-7,start_index:end_index,0,0][max_std_index])\n",
    "# print(torch.max(std_[-7,start_index:end_index,0,0]))\n",
    "# divider =   scale * (z[1][start_index:end_index][max_std_index] - z[2][start_index:end_index][max_std_index])\n",
    "# distance_1 = obj[start_index:end_index] +  divider - mean_[-7,start_index:end_index,:,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# slic_index = np.random.permutation(64)[:20]\n",
    "# print(slic_index) \n",
    "\n",
    "divider =   z[1][start_index:end_index] - z[2][start_index:end_index]\n",
    "distance_1 = obj[start_index:end_index] +  divider - mean_[sample_p,start_index:end_index,:,:]\n",
    "# print('distance_1  = ' , torch.abs(distance_1))\n",
    "distance =    distance_1  # / std_[sample_p,start_index:end_index,:,:]  #mean 取对应label\n",
    "div_max_dim = flat_index_to_tensor(torch.argmax(torch.abs(distance)))\n",
    "c,w,h = div_max_dim\n",
    "print('----------------------1-------------------------------')\n",
    "\n",
    "print('distance_max_dim  = ' , div_max_dim)\n",
    "print('distance_ max   = ' , distance[div_max_dim])\n",
    "print('distance__max  = ' , torch.max(torch.abs(distance)))\n",
    "print('----------------------2-------------------------------')\n",
    "print('std  = ' , std_[sample_p,c,w,h ])\n",
    "print('mean_a1 = ',mean_[sample_p,c,w,h ])\n",
    "u_total_t = torch.from_numpy(u_total).cuda()\n",
    "print('mean_total  = ',u_total[c,w,h ])\n",
    "print('a1  = ',obj[c,w,h])\n",
    "print('----------------------3-------------------------------')\n",
    "s_max = 1\n",
    "c,w,h = div_max_dim\n",
    "print(c,w,h)\n",
    "\n",
    "a_u_distance_p = mean_[sample_p,c,w,h ] - obj[c,w,h] + s_max * std_[sample_p, c,w,h]\n",
    "            \n",
    "\n",
    "\n",
    "scale_real_p  =  a_u_distance_p / divider[c,w,h]\n",
    "print(scale_real_p)\n",
    "a_u_distance_n = mean_[sample_p, c,w,h ] - obj[c,w,h] - s_max * std_[sample_p,  c,w,h]\n",
    "            \n",
    "scale_real_n  =  a_u_distance_n / divider[c,w,h]\n",
    "print(scale_real_n)\n",
    "\n",
    "\n",
    "if torch.abs(obj[div_max_dim] + scale_real_p * divider[div_max_dim] - u_total_t[div_max_dim]) < \\\n",
    "        torch.abs(obj[div_max_dim] + scale_real_n * divider[div_max_dim]  - u_total_t[div_max_dim]) :\n",
    "    scale_real = scale_real_p\n",
    "else :\n",
    "    scale_real = scale_real_n\n",
    "# if scale_real < -0.5 :\n",
    "#     scale_real = -0.5\n",
    "# elif scale_real > 0.5:\n",
    "#     scale_real = 0.5\n",
    "if scale_real < 0.2 and scale_real > 0:\n",
    "    scale_real = 0.2\n",
    "elif scale_real >  -0.2 and scale_real < 0:\n",
    "    scale_real = -0.2\n",
    "\n",
    "\n",
    "# scale_real = -0.3\n",
    "\n",
    "# if torch.abs(scale_real_p) < torch.abs(scale_real_n) :\n",
    "#     scale_real = scale_real_p\n",
    "# else :\n",
    "#     scale_real = scale_real_n\n",
    "# scale_real = -0.3\n",
    "print('---------------------- 4 -------------------------------')\n",
    "obj[start_index:end_index] = obj[start_index:end_index] + scale_real * divider\n",
    "print('scale_real  = ',scale_real)\n",
    "print('after scale_real   = ',obj[div_max_dim])\n",
    "\n",
    "# obj[start_index:end_index] = z[0] + scale_real_n * divider\n",
    "# print('after scale_real_n   = ',obj[div_max_dim])\n",
    "\n",
    "obj = obj.view(-1,*obj.size())\n",
    "obj_oneshot = y_onehot[0].view(-1,class_total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
